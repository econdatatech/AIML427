{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+Z4laPSI+dnb6dmm1FZNf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/econdatatech/AIML427/blob/main/Untitled12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run below commands in google colab\n",
        "# install Java8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# download spark3.0.0\n",
        "!wget -q http://apache.osuosl.org/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "# unzip it\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "# install findspark \n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOugIQ5jUrD_",
        "outputId": "f786855b-bc00-4a6d-803d-90f7188ccdaa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: spark-3.0.0-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "vwJn4BrTlcFy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 uninstall -y pyspark\n",
        "!pip3 install pyspark==3.0.2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CA80x85-Uxa_",
        "outputId": "600f553a-748a-482b-9aa2-57d26316f58d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping pyspark as it is not installed.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark==3.0.2\n",
            "  Downloading pyspark-3.0.2.tar.gz (204.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 204.8 MB 55 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 57.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.2-py2.py3-none-any.whl size=205186690 sha256=2e0dc010015cd7508cf72035dc1b7d0172cc516199b86c9fa20146610426e941\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/39/f6/970565f38054a830e9a8593f388b36e14d75dba6c6fdafc1ec\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "# Test the spark\n",
        "df = spark.createDataFrame([{\"hello\": \"world\"} for x in range(1000)])\n",
        "df.show(3, False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDDyiph7oRhU",
        "outputId": "73c783e1-4ede-4d5d-86a8-918188048067"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/session.py:381: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
            "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|hello|\n",
            "+-----+\n",
            "|world|\n",
            "|world|\n",
            "|world|\n",
            "+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from operator import add\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "start = time.time()\n",
        "#amended from https://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier\n",
        "\n",
        "seed = 2342\n",
        "# Load the data stored in csv format as a DataFrame.\n",
        "data = spark.read.load(\"kdd.data\",\n",
        "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"false\").toDF('duration', \n",
        "                      'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment',\n",
        "                      'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted',\n",
        "                      'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', \n",
        "                      'is_host_login', 'is_guest_login','count', 'srv_count', 'serror_rate', 'srv_serror_rate', \n",
        "                      'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', \n",
        "                      'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "                      'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
        "                      'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate','label')\n",
        "\n",
        "assembler = VectorAssembler().setInputCols(['duration', \n",
        "                      'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment',\n",
        "                      'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted',\n",
        "                      'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', \n",
        "                      'is_host_login', 'is_guest_login','count', 'srv_count', 'serror_rate', 'srv_serror_rate', \n",
        "                      'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', \n",
        "                      'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "                      'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
        "                      'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']).setOutputCol('features')\n",
        "\n",
        "data=assembler.transform(data)\n",
        "# Index labels, adding metadata to the label column.\n",
        "# Fit on whole dataset to include all labels in index.\n",
        "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
        "# Automatically identify categorical features, and index them.\n",
        "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "#featureIndexer = [VectorIndexer(inputCol=column, outputCol=column+\"_index\", maxCategories=4).fit(data) for column in list(set(data.columns[:-1])) ]\n",
        "featureIndexer =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
        "\n",
        "\n",
        "# Split the data into training and test sets (30% held out for testing)\n",
        "(trainingData, testData) = data.randomSplit([0.7, 0.3],seed)\n",
        "\n",
        "# Train a DecisionTree model.\n",
        "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
        "\n",
        "# Chain indexers and tree in a Pipeline\n",
        "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
        "\n",
        "# Train model.  This also runs the indexers.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions_train = model.transform(trainingData)\n",
        "\n",
        "predictions_test = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "# predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "accuracy_train = evaluator.evaluate(predictions_train)\n",
        "accuracy_test = evaluator.evaluate(predictions_test)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "columns = ['ValueType', 'Value']\n",
        "vals = [\n",
        "     ('Train accuracy', (accuracy_train)),\n",
        "     ('Test accuracy', (accuracy_test)),\n",
        "    ('Run time', (end-start)/60)\n",
        "\n",
        "]\n",
        "df = spark.createDataFrame(vals, columns)\n",
        "\n",
        "#Report the training and test results including the max, min,\n",
        "#average accuracy and the standard deviation obtained from the 10 runs,\n",
        "\n",
        "df.coalesce(1).write.csv(\"myresults\"+'/'+str(seed))\n",
        "\n"
      ],
      "metadata": {
        "id": "AYiJNFMdoZYL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from operator import add\n",
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import when, lit\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "#amended from https://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier\n",
        "\n",
        "seed = 2342\n",
        "# Load the data stored in csv format as a DataFrame.\n",
        "data = spark.read.load(\"kdd.data\",\n",
        "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"false\").toDF('duration', \n",
        "                      'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment',\n",
        "                      'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted',\n",
        "                      'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', \n",
        "                      'is_host_login', 'is_guest_login','count', 'srv_count', 'serror_rate', 'srv_serror_rate', \n",
        "                      'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', \n",
        "                      'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "                      'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
        "                      'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate','labelstr')\n",
        "\n",
        "                      \n",
        "\n",
        "data = data.withColumn('label', when(data.labelstr=='normal', \n",
        "lit('0')).otherwise('1'))\n",
        "data = data.withColumn(\"label\", data.label.cast(IntegerType()))\n",
        "\n",
        "assembler = VectorAssembler().setInputCols(['duration', \n",
        "                      'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment',\n",
        "                      'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted',\n",
        "                      'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', \n",
        "                      'is_host_login', 'is_guest_login','count', 'srv_count', 'serror_rate', 'srv_serror_rate', \n",
        "                      'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', \n",
        "                      'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "                      'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
        "                      'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']).setOutputCol('features')\n",
        "\n",
        "data=assembler.transform(data)\n",
        "# Index labels, adding metadata to the label column.\n",
        "# Fit on whole dataset to include all labels in index.\n",
        "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
        "# Automatically identify categorical features, and index them.\n",
        "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "#featureIndexer = [VectorIndexer(inputCol=column, outputCol=column+\"_index\", maxCategories=4).fit(data) for column in list(set(data.columns[:-1])) ]\n",
        "featureIndexer =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
        "\n",
        "\n",
        "# Split the data into training and test sets (30% held out for testing)\n",
        "(trainingData, testData) = data.randomSplit([0.7, 0.3],seed)\n",
        "\n",
        "#lr = LogisticRegression(regParam=0.3, elasticNetParam=0.8)\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "# Chain indexers and tree in a Pipeline\n",
        "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, lr])\n",
        "\n",
        "# Train model.  This also runs the indexers.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions_train = model.transform(trainingData)\n",
        "\n",
        "predictions_test = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "# predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "\n",
        "accuracy_train = evaluator.evaluate(predictions_train)\n",
        "accuracy_test = evaluator.evaluate(predictions_test)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "columns = ['ValueType', 'Value']\n",
        "vals = [\n",
        "     ('Train accuracy', (accuracy_train)),\n",
        "     ('Test accuracy', (accuracy_test)),\n",
        "    ('Run time', (end-start)/60)\n",
        "\n",
        "]\n",
        "df = spark.createDataFrame(vals, columns)\n",
        "\n",
        "#Report the training and test results including the max, min,\n",
        "#average accuracy and the standard deviation obtained from the 10 runs,\n",
        "\n",
        "df.coalesce(1).write.csv(\"myresults2\"+'/'+str(seed))"
      ],
      "metadata": {
        "id": "k3Qgq6n1jvio"
      },
      "execution_count": 50,
      "outputs": []
    }
  ]
}
